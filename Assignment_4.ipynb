{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Marcov Chains"
      ],
      "metadata": {
        "id": "9i68SoznU6vD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULKWo1s7U4Mx",
        "outputId": "7c872206-345b-416e-be19-f52fd5a8f629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown\n",
            "The sun was time to pack up and head home, but she knew she walked along the sun began to pack up and laughed with delight.\n",
            "The sun was a beach towel, sipping her feet as she walked along the beach. Waves crashed against the beach. Waves crashed against the peaceful moment.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "def generate(filename: str, start_words: list[str], chain_length: int, num_generated: int) -> str:\n",
        "    # Step 1: Read the text from the file\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Step 2: Tokenize the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Step 3: Create a Markov chain model\n",
        "    def create_chain(words, chain_length):\n",
        "        chain = defaultdict(list)\n",
        "        for i in range(len(words) - chain_length):\n",
        "            key = tuple(words[i:i + chain_length])\n",
        "            value = words[i + chain_length]\n",
        "            chain[key].append(value)\n",
        "        return chain\n",
        "\n",
        "    # Create the Markov chain\n",
        "    chain = create_chain(words, chain_length)\n",
        "\n",
        "    # Step 4: Generate text based on the model and starting words\n",
        "    current_words = start_words.copy()\n",
        "    generated_words = []\n",
        "\n",
        "    for _ in range(num_generated):\n",
        "        key = tuple(current_words)\n",
        "        if key in chain:\n",
        "            next_word = random.choice(chain[key])\n",
        "            generated_words.append(next_word)\n",
        "            current_words = current_words[1:] + [next_word]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    generated_text = ' '.join(generated_words)\n",
        "    return ' '.join(start_words + generated_words)\n",
        "\n",
        "# Example usage:\n",
        "filename = '/var/sample_text.txt'\n",
        "start_words = ['The', 'quick', 'brown']\n",
        "chain_length = 2\n",
        "num_generated = 20\n",
        "\n",
        "generated_sentence = generate(filename, start_words, chain_length, num_generated)\n",
        "print(generated_sentence)\n",
        "\n",
        "filename = '/var/sample_text.txt'\n",
        "start_words = ['The']\n",
        "chain_length = 1\n",
        "num_generated = 25\n",
        "\n",
        "generated_sentence = generate(filename, start_words, chain_length, num_generated)\n",
        "print(generated_sentence)\n",
        "\n",
        "start_words = ['The']\n",
        "chain_length = 1\n",
        "num_generated = 25\n",
        "\n",
        "generated_sentence = generate(filename, start_words, chain_length, num_generated)\n",
        "print(generated_sentence)"
      ]
    }
  ]
}